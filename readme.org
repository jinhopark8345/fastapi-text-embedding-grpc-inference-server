* text embedding inference install / run
#+begin_src bash
# export model=sentence-transformers/all-mpnet-base-v2
# export volume=./data

# docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.7 --model-id $model
#
# docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.7 --model-id /home/jinho/Projects/text-embedding-inference-demo/together
# git clone https://huggingface.co/Xenova/all-mpnet-base-v2

# http
docker pull ghcr.io/huggingface/text-embeddings-inference:cpu-1.7
docker run -p 8080:80 \
  -v /home/jinho/Projects/text-embedding-inference-demo:/data \
  ghcr.io/huggingface/text-embeddings-inference:cpu-1.7 \
  --model-id /data/all-mpnet-base-v2 \
  --pooling mean \
  --max-batch-tokens 512 \
  --auto-truncate

# gRPC
git clone https://huggingface.co/Xenova/all-mpnet-base-v2
docker pull ghcr.io/huggingface/text-embeddings-inference:cpu-latest-grpc
docker run -p 8080:80 \
  -v /home/jinho/Projects/text-embedding-inference-demo:/data \
  ghcr.io/huggingface/text-embeddings-inference:cpu-latest-grpc \
  --model-id /data/all-mpnet-base-v2 \
  --pooling mean \
  --max-batch-tokens 512 \
  --auto-truncate

# curl to inference server:
grpcurl -d '{"inputs": "What is Deep Learning"}' -plaintext 0.0.0.0:8080 tei.v1.Embed/Embed

# curl to fastapi server
curl -X POST http://localhost:8000/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "hello world"}'



#+end_src
